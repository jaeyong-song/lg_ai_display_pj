{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullWithVtDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            if scale_data:\n",
    "                self.scaler_x = StandardScaler().fit(X)\n",
    "                self.scaler_y = StandardScaler().fit(y)\n",
    "                X = self.scaler_x.transform(X)\n",
    "                y = self.scaler_y.transform(y)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86832"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.read_csv('../data_full/full_dataset.csv')\n",
    "len(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86832\n",
      "86832\n",
      "86832\n"
     ]
    }
   ],
   "source": [
    "full_X = full_df[['W','L','T','Vgs','Vds']].to_numpy()\n",
    "print(len(full_X))\n",
    "full_y = full_df[['Ids']].to_numpy()\n",
    "print(len(full_y))\n",
    "dataset = FullWithVtDataset(full_X, full_y)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(dataset, [69466,17366])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullMLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron for regression.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(5, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass\n",
    "        '''\n",
    "        return self.layers(x)\n",
    "\n",
    "mlp = FullMLP().to(device)\n",
    "# mlp.load_state_dict(torch.load('../checkpoint/full_vt_mlp_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, y_hat, y):\n",
    "        log_abs_y_hat = torch.log(torch.abs(y_hat)+self.eps)\n",
    "        log_abs_y = torch.log(torch.abs(y)+self.eps)\n",
    "        log_abs_delta = log_abs_y_hat - log_abs_y\n",
    "        delta = y_hat - y\n",
    "        len_delta = y_hat.size(dim=0)\n",
    "        return torch.sqrt((1/len_delta)*torch.sum((log_abs_delta/(log_abs_y+self.eps))**2)) + torch.sqrt((1/len_delta)*torch.sum((delta/(y+self.eps))**2))\n",
    "\n",
    "# loss_function = NRMSELoss()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=1e-3, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-3, \n",
    "                                              step_size_up=5, max_lr=1e-3,\n",
    "                                              gamma=0.5, mode='exp_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.408\n",
      "Loss after mini-batch  1001: 133.987\n",
      "Loss after mini-batch  2001: 35.772\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch  1001: 18.803\n",
      "Loss after mini-batch  2001: 11.760\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 8.875\n",
      "Loss after mini-batch  2001: 5.677\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 4.666\n",
      "Loss after mini-batch  2001: 3.974\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch  1001: 3.125\n",
      "Loss after mini-batch  2001: 2.906\n",
      "Starting epoch 6\n",
      "Loss after mini-batch     1: 0.006\n",
      "Loss after mini-batch  1001: 2.625\n",
      "Loss after mini-batch  2001: 2.304\n",
      "Starting epoch 7\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch  1001: 2.122\n",
      "Loss after mini-batch  2001: 2.096\n",
      "Starting epoch 8\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 1.888\n",
      "Loss after mini-batch  2001: 1.778\n",
      "Starting epoch 9\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 1.719\n",
      "Loss after mini-batch  2001: 1.540\n",
      "Starting epoch 10\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 1.485\n",
      "Loss after mini-batch  2001: 1.427\n",
      "Starting epoch 11\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 1.404\n",
      "Loss after mini-batch  2001: 1.249\n",
      "Starting epoch 12\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 1.264\n",
      "Loss after mini-batch  2001: 1.156\n",
      "Starting epoch 13\n",
      "Loss after mini-batch     1: 0.003\n",
      "Loss after mini-batch  1001: 1.091\n",
      "Loss after mini-batch  2001: 1.153\n",
      "Starting epoch 14\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 1.018\n",
      "Loss after mini-batch  2001: 1.032\n",
      "Starting epoch 15\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.973\n",
      "Loss after mini-batch  2001: 0.922\n",
      "Starting epoch 16\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.900\n",
      "Loss after mini-batch  2001: 0.903\n",
      "Starting epoch 17\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.851\n",
      "Loss after mini-batch  2001: 0.809\n",
      "Starting epoch 18\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch  1001: 0.806\n",
      "Loss after mini-batch  2001: 0.733\n",
      "Starting epoch 19\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.731\n",
      "Loss after mini-batch  2001: 0.727\n",
      "Starting epoch 20\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.726\n",
      "Loss after mini-batch  2001: 0.691\n",
      "Starting epoch 21\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.630\n",
      "Loss after mini-batch  2001: 0.694\n",
      "Starting epoch 22\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.615\n",
      "Loss after mini-batch  2001: 0.644\n",
      "Starting epoch 23\n",
      "Loss after mini-batch     1: 0.002\n",
      "Loss after mini-batch  1001: 0.602\n",
      "Loss after mini-batch  2001: 0.611\n",
      "Starting epoch 24\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.600\n",
      "Loss after mini-batch  2001: 0.584\n",
      "Starting epoch 25\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.565\n",
      "Loss after mini-batch  2001: 0.543\n",
      "Starting epoch 26\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.540\n",
      "Loss after mini-batch  2001: 0.529\n",
      "Starting epoch 27\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.510\n",
      "Loss after mini-batch  2001: 0.505\n",
      "Starting epoch 28\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.516\n",
      "Loss after mini-batch  2001: 0.462\n",
      "Starting epoch 29\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.455\n",
      "Loss after mini-batch  2001: 0.483\n",
      "Starting epoch 30\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.452\n",
      "Loss after mini-batch  2001: 0.445\n",
      "Starting epoch 31\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.440\n",
      "Loss after mini-batch  2001: 0.433\n",
      "Starting epoch 32\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.435\n",
      "Loss after mini-batch  2001: 0.417\n",
      "Starting epoch 33\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.401\n",
      "Loss after mini-batch  2001: 0.408\n",
      "Starting epoch 34\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.388\n",
      "Loss after mini-batch  2001: 0.399\n",
      "Starting epoch 35\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.382\n",
      "Loss after mini-batch  2001: 0.369\n",
      "Starting epoch 36\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.380\n",
      "Loss after mini-batch  2001: 0.356\n",
      "Starting epoch 37\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.358\n",
      "Loss after mini-batch  2001: 0.365\n",
      "Starting epoch 38\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.356\n",
      "Loss after mini-batch  2001: 0.348\n",
      "Starting epoch 39\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.346\n",
      "Loss after mini-batch  2001: 0.325\n",
      "Starting epoch 40\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.320\n",
      "Loss after mini-batch  2001: 0.332\n",
      "Starting epoch 41\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.312\n",
      "Loss after mini-batch  2001: 0.326\n",
      "Starting epoch 42\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.310\n",
      "Loss after mini-batch  2001: 0.314\n",
      "Starting epoch 43\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.306\n",
      "Loss after mini-batch  2001: 0.302\n",
      "Starting epoch 44\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.298\n",
      "Loss after mini-batch  2001: 0.292\n",
      "Starting epoch 45\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.273\n",
      "Loss after mini-batch  2001: 0.310\n",
      "Starting epoch 46\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.280\n",
      "Loss after mini-batch  2001: 0.286\n",
      "Starting epoch 47\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.268\n",
      "Loss after mini-batch  2001: 0.273\n",
      "Starting epoch 48\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.272\n",
      "Loss after mini-batch  2001: 0.272\n",
      "Starting epoch 49\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.281\n",
      "Loss after mini-batch  2001: 0.248\n",
      "Starting epoch 50\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.267\n",
      "Loss after mini-batch  2001: 0.257\n",
      "Starting epoch 51\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.261\n",
      "Loss after mini-batch  2001: 0.248\n",
      "Starting epoch 52\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.251\n",
      "Loss after mini-batch  2001: 0.253\n",
      "Starting epoch 53\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.240\n",
      "Loss after mini-batch  2001: 0.249\n",
      "Starting epoch 54\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.241\n",
      "Loss after mini-batch  2001: 0.230\n",
      "Starting epoch 55\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.240\n",
      "Loss after mini-batch  2001: 0.229\n",
      "Starting epoch 56\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.226\n",
      "Loss after mini-batch  2001: 0.233\n",
      "Starting epoch 57\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.233\n",
      "Loss after mini-batch  2001: 0.225\n",
      "Starting epoch 58\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.215\n",
      "Loss after mini-batch  2001: 0.236\n",
      "Starting epoch 59\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.223\n",
      "Loss after mini-batch  2001: 0.218\n",
      "Starting epoch 60\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.203\n",
      "Loss after mini-batch  2001: 0.223\n",
      "Starting epoch 61\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.217\n",
      "Loss after mini-batch  2001: 0.208\n",
      "Starting epoch 62\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.216\n",
      "Loss after mini-batch  2001: 0.195\n",
      "Starting epoch 63\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.208\n",
      "Loss after mini-batch  2001: 0.201\n",
      "Starting epoch 64\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.200\n",
      "Loss after mini-batch  2001: 0.201\n",
      "Starting epoch 65\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.199\n",
      "Loss after mini-batch  2001: 0.193\n",
      "Starting epoch 66\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.193\n",
      "Loss after mini-batch  2001: 0.197\n",
      "Starting epoch 67\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.191\n",
      "Loss after mini-batch  2001: 0.195\n",
      "Starting epoch 68\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.182\n",
      "Loss after mini-batch  2001: 0.188\n",
      "Starting epoch 69\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.188\n",
      "Loss after mini-batch  2001: 0.183\n",
      "Starting epoch 70\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.186\n",
      "Loss after mini-batch  2001: 0.187\n",
      "Starting epoch 71\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.193\n",
      "Loss after mini-batch  2001: 0.177\n",
      "Starting epoch 72\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.183\n",
      "Loss after mini-batch  2001: 0.179\n",
      "Starting epoch 73\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.184\n",
      "Loss after mini-batch  2001: 0.171\n",
      "Starting epoch 74\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.162\n",
      "Loss after mini-batch  2001: 0.179\n",
      "Starting epoch 75\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.179\n",
      "Loss after mini-batch  2001: 0.171\n",
      "Starting epoch 76\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.176\n",
      "Loss after mini-batch  2001: 0.167\n",
      "Starting epoch 77\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.168\n",
      "Loss after mini-batch  2001: 0.169\n",
      "Starting epoch 78\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.164\n",
      "Loss after mini-batch  2001: 0.173\n",
      "Starting epoch 79\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.157\n",
      "Loss after mini-batch  2001: 0.166\n",
      "Starting epoch 80\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.164\n",
      "Loss after mini-batch  2001: 0.158\n",
      "Starting epoch 81\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.161\n",
      "Loss after mini-batch  2001: 0.155\n",
      "Starting epoch 82\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.157\n",
      "Loss after mini-batch  2001: 0.156\n",
      "Starting epoch 83\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.163\n",
      "Loss after mini-batch  2001: 0.148\n",
      "Starting epoch 84\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.154\n",
      "Loss after mini-batch  2001: 0.155\n",
      "Starting epoch 85\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.161\n",
      "Loss after mini-batch  2001: 0.145\n",
      "Starting epoch 86\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.149\n",
      "Loss after mini-batch  2001: 0.150\n",
      "Starting epoch 87\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.153\n",
      "Loss after mini-batch  2001: 0.145\n",
      "Starting epoch 88\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.151\n",
      "Loss after mini-batch  2001: 0.145\n",
      "Starting epoch 89\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.147\n",
      "Loss after mini-batch  2001: 0.145\n",
      "Starting epoch 90\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.144\n",
      "Loss after mini-batch  2001: 0.149\n",
      "Starting epoch 91\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.145\n",
      "Loss after mini-batch  2001: 0.144\n",
      "Starting epoch 92\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.134\n",
      "Loss after mini-batch  2001: 0.152\n",
      "Starting epoch 93\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.139\n",
      "Loss after mini-batch  2001: 0.144\n",
      "Starting epoch 94\n",
      "Loss after mini-batch     1: 0.001\n",
      "Loss after mini-batch  1001: 0.141\n",
      "Loss after mini-batch  2001: 0.135\n",
      "Starting epoch 95\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.144\n",
      "Loss after mini-batch  2001: 0.130\n",
      "Starting epoch 96\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.137\n",
      "Loss after mini-batch  2001: 0.139\n",
      "Starting epoch 97\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.135\n",
      "Loss after mini-batch  2001: 0.137\n",
      "Starting epoch 98\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.137\n",
      "Loss after mini-batch  2001: 0.133\n",
      "Starting epoch 99\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.135\n",
      "Loss after mini-batch  2001: 0.135\n",
      "Starting epoch 100\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  1001: 0.131\n",
      "Loss after mini-batch  2001: 0.135\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 100): # 20 epochs at maximum\n",
    "    \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                (i + 1, current_loss / 100*32))\n",
    "            current_loss = 0.0\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp.state_dict(), '../checkpoint/baseline_mlp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.07022880e-05]\n",
      " [-3.12768816e-06]\n",
      " [ 7.43912688e-06]\n",
      " ...\n",
      " [ 5.69155527e-05]\n",
      " [ 2.38789517e-06]\n",
      " [-1.31153665e-05]]\n",
      "[[5.2425996e-05]\n",
      " [1.3121132e-11]\n",
      " [1.3733064e-06]\n",
      " ...\n",
      " [3.5919002e-05]\n",
      " [7.4131680e-07]\n",
      " [1.3121132e-11]]\n",
      "tensor(1149.9041)\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.tensor([], dtype=torch.float).to(device)\n",
    "actual = torch.tensor([], dtype=torch.float).to(device)\n",
    "with torch.no_grad():\n",
    "    mlp.eval()\n",
    "    for data in test_loader:\n",
    "        inputs, values = data\n",
    "        inputs, values = inputs.float().to(device), values.float().to(device)\n",
    "        values = values.reshape((values.shape[0], 1))\n",
    "\n",
    "        outputs = mlp(inputs)\n",
    "        # outputs = dataset.scaler_y.inverse_transform(outputs)\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "        # values = dataset.scaler_y.inverse_transform(values)\n",
    "        actual = torch.cat((actual, values), 0)\n",
    "    \n",
    "predictions = predictions.cpu().numpy()\n",
    "actual = actual.cpu().numpy()\n",
    "\n",
    "eval_loss = NRMSELoss()\n",
    "\n",
    "predictions = dataset.scaler_y.inverse_transform(predictions)\n",
    "actual = dataset.scaler_y.inverse_transform(actual)\n",
    "\n",
    "nrmse = eval_loss(torch.tensor(predictions), torch.tensor(actual))\n",
    "\n",
    "print(predictions)\n",
    "print(actual)\n",
    "\n",
    "print(nrmse)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3829caa72e4f00bfa838cc8791f96dc30ddd90ae454d4eaf3d8b7ab9877dcc00"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('lg_display')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
